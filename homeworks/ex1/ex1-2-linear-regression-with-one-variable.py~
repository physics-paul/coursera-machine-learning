"""*********************************************************

NAME:     ex1

AUTHOR:   Paul Haddon Sanders IV, Ph.D.

VERSION:  2. Linear Regression with one variable

*********************************************************"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats

###    Get data

data1 = pd.read_csv("ex1/ex1data1.txt",header=None)

### 2.2.3 Computing the cost J(theta) ######################

class linear_regression:
    
    def __init__(self,eta=0.01,n_iter=10):
        self.eta    = eta
        self.n_iter = n_iter
        
    def fit(self,x_vals,y_vals):

        self.w0 = np.random.normal(loc=0.0,scale=0.01,size=1+x_vals.shape[1])

        self.cost_ = []

        m = len(x_vals)
        
        for _ in range(self.n_iter):

            output = self.activation(self.input(x_vals))

            errors = y_vals - prediction

            self.w0[1:] += self.eta * x_vals.T.dot(errors)

            self.w0[0]  += self.eta * errors.sum()

            cost = 0.5/m*errors**2
            
            self.cost_.append(cost)

    def input(self,x_vals):

        z = np.dot(self.w0[1:],x_vals) + self.w0[0]
        
        return z
    
    def activation(self,z):
        ### linear activation ###
        return z







pb()
    
plt.plot(xvals,intercept + slope * xvals)
plt.scatter(data1.iloc[:,0],data1.iloc[:,1])
plt.show()
