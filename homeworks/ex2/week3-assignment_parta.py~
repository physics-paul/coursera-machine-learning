from sklearn.linear_model import LogisticRegression
import numpy as np
import pandas as pd
from pdb import set_trace as pb
import matplotlib.pyplot as plt
import matplotlib.animation as animation

###########################

data1 = pd.read_csv("ex2/ex2data1.txt",header=None)

x_vals = data1.iloc[:,:2].values

x_std  = np.copy(x_vals)
mean  = [x_vals[:,i].mean() for i in range(x_vals.shape[-1])] 
std   = [x_vals[:,i].std() for i in range(x_vals.shape[-1])]
x_std = (x_vals - mean) / std

y_vals = data1.iloc[:,2].values

############################

"""

plt.scatter(data1[data1[2] == 0].iloc[:,0],data1[data1[2] == 0].iloc[:,1],marker="o")
plt.scatter(data1[data1[2] == 1].iloc[:,0],data1[data1[2] == 1].iloc[:,1],marker="x")

plt.show()

"""

############################

class LogisticRegressionGD:
    
    def __init__(self,eta=0.01,n_iter=100):
        self.eta = eta
        self.n_iter = n_iter

    def predict(self,x_vals):
        
        z  = np.dot(x_vals,self.w0[1:]) + self.w0[0]
        
        return 1. / (1. + np.exp(-np.clip(z,-250,250)))
        
    def fit(self,x_vals,y_vals):

        self.w0 = np.random.normal(loc=0.0,scale=0.01,size = 1 + x_vals.shape[1])

        self.cost_ = []
        
        for _ in range(self.n_iter):

            prediction = self.predict(x_vals)

            errors = y_vals - prediction

            self.w0[1:] += self.eta * x_vals.T.dot(errors)

            self.w0[0]  += self.eta * errors.sum()

            # This is where Logistic Regression differs from Adaline
            
            cost = -y_vals.dot(np.log(prediction)) - (1-y_vals).dot(np.log(1-prediction))
            
            self.cost_.append(cost)

############################

fig, ax = plt.subplots()

ada = LogisticRegressionGD(eta=0.1,n_iter=100)
ada.fit(x_std,y_vals)

xplt = np.linspace(30,100,100)
yplt = (-ada.w0[0] - ada.w0[1] * (xplt - mean[0]) / std[0]) * std[1] / ada.w0[2] + mean[1]

line, = ax.plot(xplt,yplt)

truth = data1[2] == 0
not_truth = data1[2] == 1
ax.scatter(x_vals[truth,0],x_vals[truth,1],color="red",marker="o")
ax.scatter(x_vals[not_truth,0],x_vals[not_truth,1],color="blue",marker="x")
           
ax.set_ylim([30,100])
ax.set_xlim([30,100])
plt.xlabel('Exam1 (std)')
plt.ylabel('Exam2 (std)')

def animate(i):

    ada = LogisticRegressionGD(eta=0.1,n_iter=2+i)

    ada.fit(x_std,y_vals)
    
    ydat = (-ada.w0[0] - ada.w0[1] * (xplt - mean[0]) / std[0]) * std[1] / ada.w0[2] + mean[1]
    
    line.set_ydata(ydat)
    
    return line,

ani = animation.FuncAnimation(fig,animate,frames=200,interval=25)

plt.show(block=False)

scores = (np.array([45,85]) - mean) / std

print("Prediction for Exam 1: 45 and Exam 2: 85 : {}".format(ada.predict(scores)))
